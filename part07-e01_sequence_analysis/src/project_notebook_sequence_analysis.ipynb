{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Analysis with Python\n",
    "\n",
    "Contact: Veli MÃ¤kinen veli.makinen@helsinki.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following assignments introduce applications of hashing with ```dict()``` primitive of Python. While doing so, a rudimentary introduction to biological sequences is given. \n",
    "This framework is then enhanced with probabilities, leading to routines to generate random sequences under some constraints, including a general concept of *Markov-chains*. All these components illustrate the usage of ```dict()```, but at the same time introduce some other computational routines to efficiently deal with probabilities.   \n",
    "The function ```collections.defaultdict``` can be useful.\n",
    "\n",
    "Below are some \"suggested\" imports. Feel free to use and modify these, or not. Generally it's good practice to keep most or all imports in one place. Typically very close to the start of notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.831112Z",
     "start_time": "2019-07-08T22:04:22.688031Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automated TMC tests do not test cell outputs. These are intended to be evaluated in the peer reviews. So it is still be a good idea to make the outputs as clear and informative as possible.\n",
    "\n",
    "To keep TMC tests running as well as possible it is recommended to keep global variable assignments in the notebook to a minimum to avoid potential name clashes and confusion. Additionally you should keep all actual code exection in main guards to keep the test running smoothly. If you run [check_sequence.py](https://raw.githubusercontent.com/saskeli/data-analysis-with-python-summer-2019/master/check_outputs.py) in the `part07-e01_sequence_analysis` folder, the script should finish very quickly and optimally produce no output.\n",
    "\n",
    "If you download data from the internet during execution (codon usage table), the parts where downloading is done should not work if you decide to submit to the tmc server. Local tests should work fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNA and RNA\n",
    "\n",
    "A DNA molecule consist, in principle, of a chain of smaller molecules. These smaller molecules have some common basic components (bases) that repeat. For our purposes it is sufficient to know that these bases are nucleotides adenine, cytosine, guanine, and thymine with abbreviations ```A```, ```C```, ```G```, and ```T```. Given a *DNA sequence* e.g. ```ACGATGAGGCTCAT```, one can reverse engineer (with negligible loss of information) the corresponding DNA molecule.\n",
    "\n",
    "Parts of a DNA molecule can *transcribe* into an RNA molecule. In this process, thymine gets replaced by uracil (```U```). \n",
    "\n",
    "\n",
    "1. Write a function ```dna_to_rna``` to convert a given DNA sequence $s$ into an RNA sequence. For the sake of exercise, use ```dict()``` to store the symbol to symbol encoding rules. Create a program to test your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.841952Z",
     "start_time": "2019-07-08T22:04:22.834721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AACGUGAUUUC\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dna_to_rna(s):\n",
    "    transcription_rules = {\n",
    "        'A': 'A',\n",
    "        'C': 'C',\n",
    "        'G': 'G',\n",
    "        'T': 'U'\n",
    "    }\n",
    "    return \"\".join(transcription_rules[nucleotide] for nucleotide in s)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print(dna_to_rna(\"AACGTGATTTC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "\n",
    "The solution involves defining a dictionary that maps each DNA nucleotide (A, C, G, T) to its corresponding RNA nucleotide (A, C, G, U), then using a list comprehension within the dna_to_rna function to iterate over each nucleotide in the input DNA sequence, replacing it according to the dictionary, and finally joining the resulting list of RNA nucleotides into a single string to form the RNA sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Expected output: AACGUGAUUUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proteins\n",
    "\n",
    "Like DNA and RNA, protein molecule can be interpreted as a chain of smaller molecules, where the bases are now amino acids. RNA molecule may *translate* into a protein molecule, but instead of base by base, three bases of RNA correspond to one base of protein. That is, RNA sequence is read triplet (called codon) at a time. \n",
    "\n",
    "2. Consider the codon to amino acid conversion table in http://htmlpreview.github.io/?https://github.com/csmastersUH/data_analysis_with_python_2020/blob/master/Codon%20usage%20table.html. Write a function ```get_dict``` to read the table into a ```dict()```, such that for each RNA sequence of length 3, say $\\texttt{AGU}$, the hash table stores the conversion rule to the corresponding amino acid. You may store the html page to your local src directory,\n",
    "and parse that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.867855Z",
     "start_time": "2019-07-08T22:04:22.845885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UUU': 'F', 'UCU': 'S', 'UAU': 'Y', 'UGU': 'C', 'UUC': 'F', 'UCC': 'S', 'UAC': 'Y', 'UGC': 'C', 'UUA': 'L', 'UCA': 'S', 'UAA': '*', '40285)': 'UGA', '(': '63237)', 'UUG': 'L', 'UCG': 'S', 'UAG': '*', '32109)': 'UGG', 'CUU': 'L', 'CCU': 'P', 'CAU': 'H', 'CGU': 'R', 'CUC': 'L', 'CCC': 'P', 'CAC': 'H', 'CGC': 'R', 'CUA': 'L', 'CCA': 'P', 'CAA': 'Q', 'CGA': 'R', 'CUG': 'L', 'CCG': 'P', 'CAG': 'Q', 'CGG': 'R', 'AUU': 'I', 'ACU': 'T', 'AAU': 'N', 'AGU': 'S', 'AUC': 'I', 'ACC': 'T', 'AAC': 'N', 'AGC': 'S', 'AUA': 'I', 'ACA': 'T', 'AAA': 'K', 'AGA': 'R', 'AUG': 'M', 'ACG': 'T', 'AAG': 'K', 'AGG': 'R', 'GUU': 'V', 'GCU': 'A', 'GAU': 'D', 'GGU': 'G', 'GUC': 'V', 'GCC': 'A', 'GAC': 'D', 'GGC': 'G', 'GUA': 'V', 'GCA': 'A', 'GAA': 'E', 'GGA': 'G', 'GUG': 'V', 'GCG': 'A', 'GAG': 'E', 'GGG': 'G'}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "FILE_PATH = Path(\"src\") / \"codon_table.html\"\n",
    "\n",
    "def get_dict():\n",
    "    with open(\"/Users/vynguyen/Library/Application Support/tmc/vscode/mooc-data-analysis-with-python-2023-2024/part07-e01_sequence_analysis/src/codon_table.html\", \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    codon_to_aa = {}\n",
    "    \n",
    "    pre_tag = soup.find('pre')\n",
    "\n",
    "    lines = pre_tag.text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        items = line.split()\n",
    "        for i in range(0, len(items), 5):\n",
    "            if i + 1 < len(items):\n",
    "                codon = items[i]\n",
    "                amino_acid = items[i + 1]\n",
    "                codon_to_aa[codon] = amino_acid\n",
    "\n",
    "    return codon_to_aa\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    codon_to_aa = get_dict()\n",
    "    print(codon_to_aa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "- File Reading: The script reads the HTML content from the specified file path.\n",
    "- HTML Parsing: It uses BeautifulSoup to parse the HTML content and locate the `<pre>` tag.\n",
    "- Text Processing: The text within the `<pre>` tag is stripped of leading/trailing whitespace and split into lines.\n",
    "- Data Extraction: Each line is split into items. Codon and amino acid pairs are identified and added to a dictionary.\n",
    "- **Result Output:** The resulting dictionary mapping codons to amino acids is printed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Make sure to replace \"/path/to/your/codon_table.html\" with the actual path to your codon_table.html file.</br>\n",
    "The result is:</br>\n",
    "{'UUU': 'F', 'UCU': 'S', 'UAU': 'Y', 'UGU': 'C', 'UUC': 'F', 'UCC': 'S', 'UAC': 'Y', 'UGC': 'C', 'UUA': 'L', 'UCA': 'S', 'UAA': '*', 'UGA': '*', 'UUG': 'L', 'UCG': 'S', 'UAG': '*', 'UGG': 'W', 'CUU': 'L', 'CCU': 'P', 'CAU': 'H', 'CGU': 'R', 'CUC': 'L', 'CCC': 'P', 'CAC': 'H', 'CGC': 'R', 'CUA': 'L', 'CCA': 'P', 'CAA': 'Q', 'CGA': 'R', 'CUG': 'L', 'CCG': 'P', 'CAG': 'Q', 'CGG': 'R', 'AUU': 'I', 'ACU': 'T', 'AAU': 'N', 'AGU': 'S', 'AUC': 'I', 'ACC': 'T', 'AAC': 'N', 'AGC': 'S', 'AUA': 'I', 'ACA': 'T', 'AAA': 'K', 'AGA': 'R', 'AUG': 'M', 'ACG': 'T', 'AAG': 'K', 'AGG': 'R', 'GUU': 'V', 'GCU': 'A', 'GAU': 'D', 'GGU': 'G', 'GUC': 'V', 'GCC': 'A', 'GAC': 'D', 'GGC': 'G', 'GUA': 'V', 'GCA': 'A', 'GAA': 'E', 'GGA': 'G', 'GUG': 'V', 'GCG': 'A', 'GAG': 'E', 'GGG': 'G'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the same conversion table as above, but now write function `get_dict_list` to read the table into a `dict()`, such that for each amino acid the hash table stores the list of codons encoding it.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.882386Z",
     "start_time": "2019-07-08T22:04:22.872449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'F': ['UUU', 'UUC'], 'S': ['UCU', 'UCC', 'UCA', 'UCG', 'AGU', 'AGC'], 'Y': ['UAU', 'UAC'], 'C': ['UGU', 'UGC'], 'L': ['UUA', 'UUG', 'CUU', 'CUC', 'CUA', 'CUG'], '*': ['UAA', 'UAG'], 'UGA': ['40285)'], '63237)': ['('], 'UGG': ['32109)'], 'P': ['CCU', 'CCC', 'CCA', 'CCG'], 'H': ['CAU', 'CAC'], 'R': ['CGU', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'], 'Q': ['CAA', 'CAG'], 'I': ['AUU', 'AUC', 'AUA'], 'T': ['ACU', 'ACC', 'ACA', 'ACG'], 'N': ['AAU', 'AAC'], 'K': ['AAA', 'AAG'], 'M': ['AUG'], 'V': ['GUU', 'GUC', 'GUA', 'GUG'], 'A': ['GCU', 'GCC', 'GCA', 'GCG'], 'D': ['GAU', 'GAC'], 'G': ['GGU', 'GGC', 'GGA', 'GGG'], 'E': ['GAA', 'GAG']}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_dict_list():\n",
    "    with open(\"/Users/vynguyen/Library/Application Support/tmc/vscode/mooc-data-analysis-with-python-2023-2024/part07-e01_sequence_analysis/src/codon_table.html\", \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    codon_to_aa = {}\n",
    "    \n",
    "    pre_tag = soup.find('pre')\n",
    "\n",
    "    lines = pre_tag.text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        items = line.split()\n",
    "        for i in range(0, len(items), 5):\n",
    "            if i + 1 < len(items):\n",
    "                codon = items[i]\n",
    "                amino_acid = items[i + 1]\n",
    "                if amino_acid not in codon_to_aa:\n",
    "                    codon_to_aa[amino_acid] = []\n",
    "                codon_to_aa[amino_acid].append(codon)\n",
    "\n",
    "    return codon_to_aa\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    aa_to_codon_list = get_dict_list()\n",
    "    print(aa_to_codon_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Everything is pretty much similar to the problem 2 except this part </br>\n",
    "**Data Extraction**: Each line is split into items. Codon and amino acid pairs are identified. For each amino acid, the corresponding codons are added to a list in a dictionary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Expected result:\n",
    "\n",
    "{'F': ['UUU', 'UUC'], 'S': ['UCU', 'UCC', 'UCA', 'UCG', 'AGU', 'AGC'], 'Y': ['UAU', 'UAC'], 'C': ['UGU', 'UGC'], 'L': ['UUA', 'UUG', 'CUU', 'CUC', 'CUA', 'CUG'], '*': ['UAA', 'UAG'], 'UGA': ['40285)'], '63237)': ['('], 'UGG': ['32109)'], 'P': ['CCU', 'CCC', 'CCA', 'CCG'], 'H': ['CAU', 'CAC'], 'R': ['CGU', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'], 'Q': ['CAA', 'CAG'], 'I': ['AUU', 'AUC', 'AUA'], 'T': ['ACU', 'ACC', 'ACA', 'ACG'], 'N': ['AAU', 'AAC'], 'K': ['AAA', 'AAG'], 'M': ['AUG'], 'V': ['GUU', 'GUC', 'GUA', 'GUG'], 'A': ['GCU', 'GCC', 'GCA', 'GCG'], 'D': ['GAU', 'GAC'], 'G': ['GGU', 'GGC', 'GGA', 'GGG'], 'E': ['GAA', 'GAG']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the conversion tables at hand, the following should be trivial to solve.\n",
    "\n",
    "4. Fill in function ```rna_to_prot``` in the stub solution to convert a given DNA sequence $s$ into a protein sequence. \n",
    "You may use the dictionaries from exercises 2 and 3. You can test your program with `ATGATATCATCGACGATGTAG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.913321Z",
     "start_time": "2019-07-08T22:04:22.906646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSTM*\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "CODON_TO_AA_DICT = {'UUU': 'F', 'UCU': 'S', 'UAU': 'Y', 'UGU': 'C', 'UUC': 'F', 'UCC': 'S', 'UAC': 'Y', 'UGC': 'C', 'UUA': 'L', 'UCA': 'S', 'UAA': '*', '40285)': 'UGA', '(': '63237)', 'UUG': 'L', 'UCG': 'S', 'UAG': '*', '32109)': 'UGG', 'CUU': 'L', 'CCU': 'P', 'CAU': 'H', 'CGU': 'R', 'CUC': 'L', 'CCC': 'P', 'CAC': 'H', 'CGC': 'R', 'CUA': 'L', 'CCA': 'P', 'CAA': 'Q', 'CGA': 'R', 'CUG': 'L', 'CCG': 'P', 'CAG': 'Q', 'CGG': 'R', 'AUU': 'I', 'ACU': 'T', 'AAU': 'N', 'AGU': 'S', 'AUC': 'I', 'ACC': 'T', 'AAC': 'N', 'AGC': 'S', 'AUA': 'I', 'ACA': 'T', 'AAA': 'K', 'AGA': 'R', 'AUG': 'M', 'ACG': 'T', 'AAG': 'K', 'AGG': 'R', 'GUU': 'V', 'GCU': 'A', 'GAU': 'D', 'GGU': 'G', 'GUC': 'V', 'GCC': 'A', 'GAC': 'D', 'GGC': 'G', 'GUA': 'V', 'GCA': 'A', 'GAA': 'E', 'GGA': 'G', 'GUG': 'V', 'GCG': 'A', 'GAG': 'E', 'GGG': 'G'}\n",
    "\n",
    "def dna_to_rna(dna_sequence):\n",
    "    return dna_sequence.replace('T', 'U')\n",
    "\n",
    "def rna_to_prot(rna_sequence):\n",
    "    codon_to_aa = CODON_TO_AA_DICT\n",
    "    protein_sequence = []\n",
    "    \n",
    "    for i in range(0, len(rna_sequence) - 2, 3):\n",
    "        codon = rna_sequence[i:i+3]\n",
    "        protein_sequence.append(codon_to_aa.get(codon, '?'))\n",
    "    \n",
    "    return ''.join(protein_sequence)\n",
    "\n",
    "def dna_to_prot(dna_sequence):\n",
    "    rna_sequence = dna_to_rna(dna_sequence)\n",
    "    return rna_to_prot(rna_sequence)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(dna_to_prot(\"ATGATATCATCGACGATGTAG\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "rna_to_prot Function: Converts an RNA sequence into a protein sequence. It processes the RNA sequence in triplets (codons) and uses the codon-to-amino-acid dictionary to translate each codon to an amino acid. If a codon is not found in the dictionary, it appends a '?' to denote an unknown amino acid.</br>\n",
    "\n",
    "dna_to_prot Function: Converts a DNA sequence to RNA and then translates the RNA to a protein sequence using the rna_to_prot function.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Expected result:<br/>\n",
    "MISSTM*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there are $4^3=64$ different codons, but only 20 amino acids. That is, some triplets encode the same amino acid.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse translation\n",
    "\n",
    "It has been observed that among the codons coding the same amino acid, some are more frequent than others. These frequencies can be converted to probabilities. E.g. consider codons `AUU`, `AUC`, and `AUA` that code for amino acid isoleucine.\n",
    "If they are observed, say, 36, 47, 17 times, respectively, to code isoleucine in a dataset, the probability that a random such event is `AUU` $\\to$ isoleucine is 36/100.\n",
    "\n",
    "This phenomenon is called *codon adaptation*, and for our purposes it works as a good introduction to generation of random sequences under constraints.   \n",
    "\n",
    "5. Consider the codon adaptation frequencies in http://htmlpreview.github.io/?https://github.com/csmastersUH/data_analysis_with_python_2020/blob/master/Codon%20usage%20table.html and read them into a ```dict()```, such that for each RNA sequence of length 3, say `AGU`, the hash table stores the probability of that codon among codons encoding the same amino acid.\n",
    "Put your solution in the ```get_probabability_dict``` function. Use the column \"([number])\" to estimate the probabilities, as the two preceding columns contain truncated values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:22.966173Z",
     "start_time": "2019-07-08T22:04:22.956013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UUU': 0.8846153846153846, 'UUC': 0.11538461538461539, 'UUA': 0.15966386554621848, 'UUG': 0.01680672268907563, 'CUU': 0.3697478991596639, 'CUC': 0.01680672268907563, 'CUA': 0.3865546218487395, 'CUG': 0.05042016806722689, 'AUU': 0.6835443037974683, 'AUC': 0.0379746835443038, 'AUA': 0.27848101265822783, 'AUG': 1.0, 'GUU': 0.47058823529411764, 'GUC': 0.025210084033613446, 'GUA': 0.453781512605042, 'GUG': 0.05042016806722689}\n",
      "AUA: 0.278481\tAUC: 0.037975\tAUG: 1.000000\tAUU: 0.683544\tCUA: 0.386555\tCUC: 0.016807\n",
      "CUG: 0.050420\tCUU: 0.369748\tGUA: 0.453782\tGUC: 0.025210\tGUG: 0.050420\tGUU: 0.470588\n",
      "UUA: 0.159664\tUUC: 0.115385\tUUG: 0.016807\tUUU: 0.884615\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Regular expressions for parsing the HTML file\n",
    "CODON_REGEX = r'([A-Z]{3})\\s+'\n",
    "AA_REGEX = r'\\s+([A-Z*]{1})\\s+'\n",
    "FREQUENCY_REGEX = r'(\\d+)\\s+'\n",
    "\n",
    "def get_probabability_dict():\n",
    "    with open(\"/Users/vynguyen/Library/Application Support/tmc/vscode/mooc-data-analysis-with-python-2023-2024/part07-e01_sequence_analysis/src/codon_table.html\", \"r\") as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    pre_tag = soup.find('pre') \n",
    "    parsed = pre_tag.text\n",
    "    \n",
    "    # Extract codons, amino acids, and frequencies using regular expressions\n",
    "    codons = re.findall(CODON_REGEX, parsed)\n",
    "    aminoacids = re.findall(AA_REGEX, parsed)\n",
    "    frequencies = [int(freq) for freq in re.findall(FREQUENCY_REGEX, parsed)]\n",
    "\n",
    "    # Combine codons and frequencies for each amino acid\n",
    "    codon_freq_dict = {}\n",
    "    for i in range(0, len(codons), 4):\n",
    "        codon = codons[i]\n",
    "        amino_acid = aminoacids[i]\n",
    "        freq = frequencies[i // 4]\n",
    "        if amino_acid not in codon_freq_dict:\n",
    "            codon_freq_dict[amino_acid] = {}\n",
    "        codon_freq_dict[amino_acid][codon] = freq\n",
    "\n",
    "    # Calculate probabilities\n",
    "    prob_dict = {}\n",
    "    for amino_acid, codon_freqs in codon_freq_dict.items():\n",
    "        total_freq = sum(codon_freqs.values())\n",
    "        for codon, freq in codon_freqs.items():\n",
    "            prob_dict[codon] = freq / total_freq\n",
    "\n",
    "    return prob_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    codon_to_prob = get_probabability_dict()\n",
    "    items = sorted(codon_to_prob.items(), key=lambda x: x[0])\n",
    "    print(codon_to_prob)\n",
    "    for i in range(1 + len(items) // 6):\n",
    "        print(\"\\t\".join(\n",
    "            f\"{k}: {v:.6f}\"\n",
    "            for k, v in items[i * 6:6 + i * 6]\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "In reverse translation, I need to account for the fact that multiple codons can encode the same amino acid, making it difficult to determine the exact codon from the amino acid alone. I start by reading and parsing the HTML file that contains codon frequencies using BeautifulSoup. I then extract codons, amino acids, and their frequencies using regex. I compile this information into a dictionary, where each amino acid maps to a list of codons and their frequencies. To calculate the probability of each codon, I sum the frequencies of all codons for each amino acid and then divide the frequency of each codon by this sum to obtain the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The reverse dict: <br/>\n",
    "{'UUU': 0.8846153846153846, 'UUC': 0.11538461538461539, 'UUA': 0.15966386554621848, 'UUG': 0.01680672268907563, 'CUU': 0.3697478991596639, 'CUC': 0.01680672268907563, 'CUA': 0.3865546218487395, 'CUG': 0.05042016806722689, 'AUU': 0.6835443037974683, 'AUC': 0.0379746835443038, 'AUA': 0.27848101265822783, 'AUG': 1.0, 'GUU': 0.47058823529411764, 'GUC': 0.025210084033613446, 'GUA': 0.453781512605042, 'GUG': 0.05042016806722689}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have everything in place to easily solve the following.\n",
    "\n",
    "\n",
    "6. Write a class ```ProteinToMaxRNA``` with a ```convert``` method which converts a protein sequence into the most likely RNA sequence to be the source of this protein. Run your program with `LTPIQNRA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.000743Z",
     "start_time": "2019-07-08T22:04:22.992108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUAAUU\n"
     ]
    }
   ],
   "source": [
    "# Define the codon probabilities and amino acid to codon mapping\n",
    "CODON_PROB = {'UUU': 0.8846153846153846, 'UUC': 0.11538461538461539, 'UUA': 0.15966386554621848, 'UUG': 0.01680672268907563, 'CUU': 0.3697478991596639, 'CUC': 0.01680672268907563, 'CUA': 0.3865546218487395, 'CUG': 0.05042016806722689, 'AUU': 0.6835443037974683, 'AUC': 0.0379746835443038, 'AUA': 0.27848101265822783, 'AUG': 1.0, 'GUU': 0.47058823529411764, 'GUC': 0.025210084033613446, 'GUA': 0.453781512605042, 'GUG': 0.05042016806722689}\n",
    "\n",
    "AA_TO_RNA = {'F': ['UUU', 'UUC'], 'S': ['UCU', 'UCC', 'UCA', 'UCG', 'AGU', 'AGC'], 'Y': ['UAU', 'UAC'], 'C': ['UGU', 'UGC'], 'L': ['UUA', 'UUG', 'CUU', 'CUC', 'CUA', 'CUG'], '*': ['UAA', 'UAG'], 'UGA': ['40285)'], '63237)': ['('], 'UGG': ['32109)'], 'P': ['CCU', 'CCC', 'CCA', 'CCG'], 'H': ['CAU', 'CAC'], 'R': ['CGU', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'], 'Q': ['CAA', 'CAG'], 'I': ['AUU', 'AUC', 'AUA'], 'T': ['ACU', 'ACC', 'ACA', 'ACG'], 'N': ['AAU', 'AAC'], 'K': ['AAA', 'AAG'], 'M': ['AUG'], 'V': ['GUU', 'GUC', 'GUA', 'GUG'], 'A': ['GCU', 'GCC', 'GCA', 'GCG'], 'D': ['GAU', 'GAC'], 'G': ['GGU', 'GGC', 'GGA', 'GGG'], 'E': ['GAA', 'GAG']}\n",
    "\n",
    "\n",
    "class ProteinToMaxRNA:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.codon_to_prob = CODON_PROB\n",
    "        self.amino_acid_to_max_codon = self.get_max_codon_dict()\n",
    "    \n",
    "    def get_max_codon_dict(self):\n",
    "        # Map each amino acid to the most probable codon\n",
    "        aa_to_max_codon = {}\n",
    "        for aa, codons in AA_TO_RNA.items():\n",
    "            max_prob = 0\n",
    "            max_codon = ''\n",
    "            for codon in codons:\n",
    "                prob = self.codon_to_prob.get(codon, 0)\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_codon = codon\n",
    "            aa_to_max_codon[aa] = max_codon\n",
    "        return aa_to_max_codon\n",
    "    \n",
    "    def convert(self, s):\n",
    "        rna_sequence = ''.join(self.amino_acid_to_max_codon.get(aa, 'NNN') for aa in s)\n",
    "        return rna_sequence\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    protein_to_rna = ProteinToMaxRNA()\n",
    "    print(protein_to_rna.convert(\"LTPIQNRA\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "Mapping Function: The get_max_codon_dict method maps each amino acid to the codon with the highest probability using the CODON_PROB dictionary.</br>\n",
    "\n",
    "Conversion: The convert method translates each amino acid in the protein sequence to its corresponding most probable RNA codon and concatenates them into the final RNA sequence.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "print(protein_to_rna.convert(\"LTPIQNRA\"))<br/>\n",
    "Expected result: CUAAUU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are almost ready to produce random RNA sequences that code a given protein sequence. For this, we need a subroutine to *sample from a probability distribution*. Consider our earlier example of probabilities 36/100, 47/100, and 17/100 for `AUU`, `AUC`, and `AUA`, respectively. \n",
    "Let us assume we have a random number generator ```random()``` that returns a random number from interval $[0,1)$. We may then partition the unit interval according to cumulative probabilities to $[0,36/100), [36/100,83/100), [83/100,1)$, respectively. Depending which interval the number ```random()``` hits, we select the codon accordingly.\n",
    "\n",
    "7. Write a function ```random_event``` that chooses a random event, given a probability distribution (set of events whose probabilities sum to 1).\n",
    "You can use function ```random.uniform``` to produce values uniformly at random from the range $[0,1)$. The distribution should be given to your function as a dictionary from events to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.036655Z",
     "start_time": "2019-07-08T22:04:23.030067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T, C, T, T, G, G, T, T, C, T, T, C, C, C, T, T, T, C, T, T, C, C, A, T, T, A, T, G, T\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def random_event(dist):\n",
    "    \"\"\"\n",
    "    Takes as input a dictionary from events to their probabilities.\n",
    "    Returns a random event sampled according to the given distribution.\n",
    "    The probabilities must sum to 1.0\n",
    "    \"\"\"\n",
    "    rnd = random.uniform(0, 1)\n",
    "    \n",
    "    cumulative_prob = 0.0\n",
    "    for event, prob in dist.items():\n",
    "        cumulative_prob += prob\n",
    "        if rnd < cumulative_prob:\n",
    "            return event\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    distribution = dict(zip(\"ACGT\", [0.10, 0.35, 0.15, 0.40]))\n",
    "    print(\", \".join(random_event(distribution) for _ in range(29)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The random_event function selects a random event from a given probability distribution by generating a random number between 0 and 1. It then determines which event the random number falls into based on cumulative probabilities. By summing the probabilities and comparing the random number with these cumulative values, the function identifies and returns the event corresponding to the interval in which the random number falls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Given the distribution {'A': 0.10, 'C': 0.35, 'G': 0.15, 'T': 0.40}, the cumulative probabilities are:\n",
    "\n",
    "A: [0, 0.10)\n",
    "C: [0.10, 0.45)\n",
    "G: [0.45, 0.60)\n",
    "T: [0.60, 1.00)\n",
    "The function will use these intervals to determine which event to return based on the generated random number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this general routine, the following should be easy to solve.\n",
    " \n",
    "8. Write a class ```ProteinToRandomRNA``` to produce a random RNA sequence encoding the input protein sequence according to the input codon adaptation probabilities. The actual conversion is done through the ```convert``` method. Run your program with `LTPIQNRA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.073660Z",
     "start_time": "2019-07-08T22:04:23.067966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUAACUCCGAUUCAAAAUAGAGCG\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "CODON_PROB = {'UUU': 0.8846153846153846, 'UUC': 0.11538461538461539, 'UUA': 0.15966386554621848, 'UUG': 0.01680672268907563, 'CUU': 0.3697478991596639, 'CUC': 0.01680672268907563, 'CUA': 0.3865546218487395, 'CUG': 0.05042016806722689, 'AUU': 0.6835443037974683, 'AUC': 0.0379746835443038, 'AUA': 0.27848101265822783, 'AUG': 1.0, 'GUU': 0.47058823529411764, 'GUC': 0.025210084033613446, 'GUA': 0.453781512605042, 'GUG': 0.05042016806722689}\n",
    "\n",
    "AA_TO_RNA = {'F': ['UUU', 'UUC'], 'S': ['UCU', 'UCC', 'UCA', 'UCG', 'AGU', 'AGC'], 'Y': ['UAU', 'UAC'], 'C': ['UGU', 'UGC'], 'L': ['UUA', 'UUG', 'CUU', 'CUC', 'CUA', 'CUG'], '*': ['UAA', 'UAG'], 'UGA': ['40285)'], '63237)': ['('], 'UGG': ['32109)'], 'P': ['CCU', 'CCC', 'CCA', 'CCG'], 'H': ['CAU', 'CAC'], 'R': ['CGU', 'CGC', 'CGA', 'CGG', 'AGA', 'AGG'], 'Q': ['CAA', 'CAG'], 'I': ['AUU', 'AUC', 'AUA'], 'T': ['ACU', 'ACC', 'ACA', 'ACG'], 'N': ['AAU', 'AAC'], 'K': ['AAA', 'AAG'], 'M': ['AUG'], 'V': ['GUU', 'GUC', 'GUA', 'GUG'], 'A': ['GCU', 'GCC', 'GCA', 'GCG'], 'D': ['GAU', 'GAC'], 'G': ['GGU', 'GGC', 'GGA', 'GGG'], 'E': ['GAA', 'GAG']}\n",
    "\n",
    "\n",
    "def random_event(dist):\n",
    "    \"\"\"\n",
    "    Takes as input a dictionary from events to their probabilities.\n",
    "    Returns a random event sampled according to the given distribution.\n",
    "    The probabilities must sum to 1.0\n",
    "    \"\"\"\n",
    "    rnd = random.uniform(0, 1)\n",
    "    \n",
    "    cumulative_prob = 0.0\n",
    "    for event, prob in dist.items():\n",
    "        cumulative_prob += prob\n",
    "        if rnd < cumulative_prob:\n",
    "            return event\n",
    "\n",
    "class ProteinToRandomRNA(object):\n",
    "    def __init__(self):\n",
    "        self.codon_probs = CODON_PROB\n",
    "        self.aa_to_rna = AA_TO_RNA\n",
    "\n",
    "    def __get_random_codon_from_distribution(self, aa):\n",
    "        candidate_codons = self.aa_to_rna[aa]\n",
    "        candidate_probs = {codon: self.codon_probs.get(codon, 1 / len(candidate_codons)) for codon in candidate_codons}\n",
    "        total_prob = sum(candidate_probs.values())\n",
    "        candidate_probs = {codon: prob / total_prob for codon, prob in candidate_probs.items()}\n",
    "        return random_event(candidate_probs)\n",
    "\n",
    "    def convert(self, s):\n",
    "        random_rna_sequence = [self.__get_random_codon_from_distribution(aa) for aa in s]\n",
    "        return \"\".join(random_rna_sequence)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    protein_to_random_rna = ProteinToRandomRNA()\n",
    "    print(protein_to_random_rna.convert(\"LTPIQNRA\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "To address the problem of converting a protein sequence to a random RNA sequence based on codon adaptation probabilities, we need to ensure we correctly handle the amino acid to codon mapping and the associated probabilities. We'll start by defining the codon probabilities and the amino acid to RNA mappings. Next, we'll create a function to randomly select an event based on these probabilities. The `ProteinToRandomRNA` class will include methods to initialize with these mappings, normalize the probabilities, and handle missing codons. The `convert` method will iterate through the protein sequence and convert each amino acid to a corresponding codon, forming the RNA sequence. By testing with a sample protein sequence, we can verify the correct implementation of this process. This approach ensures the selection of codons is based on their actual probabilities and handles cases where specific codons might be missing from the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "fill in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating DNA sequences with higher-order Markov chains\n",
    "\n",
    "We will now reuse the machinery derived above in a related context. We go back to DNA sequences, and consider some easy statistics that can be used to characterize the sequences. \n",
    "First, just the frequencies of bases $\\texttt{A}$, $\\texttt{C}$, $\\texttt{G}$, $\\texttt{T}$ may reveal the species from which the input DNA originates; each species has a different base composition that has been formed during evolution. \n",
    "More interestingly, the areas where DNA to RNA transcription takes place (coding region) have an excess of $\\texttt{C}$ and $\\texttt{G}$ over $\\texttt{A}$ and $\\texttt{T}$. To detect such areas a common routine is to just use a *sliding window* of fixed size, say $k$, and compute for each window position \n",
    "$T[i..i+k-1]$ the base frequencies, where $T[1..n]$ is the input DNA sequence. When sliding the window from  $T[i..i+k-1]$ to $T[i+1..i+k]$ frequency $f(T[i])$ gets decreases by one and $f(T[i+k])$ gets increased by one. \n",
    "\n",
    "9. Write a *generator* ```sliding_window``` to compute sliding window base frequencies so that each moving of the window takes constant time. We saw in the beginning of the course one way how to create generators using\n",
    "  generator expression. Here we use a different way. For the function ```sliding_window``` to be a generator, it must have at least   one ```yield``` expression, see [https://docs.python.org/3/reference/expressions.html#yieldexpr](https://docs.python.org/3/reference/expressions.html#yieldexpr).\n",
    "  \n",
    "  Here is an example of a generator expression that works similarily to the built in `range` generator:\n",
    "  ```Python\n",
    "  def range(a, b=None, c=1):\n",
    "      current = 0 if b == None else a\n",
    "      end = a if b == None else b\n",
    "      while current < end:\n",
    "          yield current\n",
    "          current += c\n",
    "  ```\n",
    "  A yield expression can be used to return a value and *temporarily* return from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.111365Z",
     "start_time": "2019-07-08T22:04:23.100858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'C': 3, 'G': 0, 'T': 1}\n",
      "{'A': 0, 'C': 3, 'G': 1, 'T': 0}\n",
      "{'A': 1, 'C': 2, 'G': 1, 'T': 0}\n",
      "{'A': 1, 'C': 2, 'G': 1, 'T': 0}\n",
      "{'A': 1, 'C': 1, 'G': 2, 'T': 0}\n",
      "{'A': 1, 'C': 1, 'G': 2, 'T': 0}\n",
      "{'A': 0, 'C': 2, 'G': 2, 'T': 0}\n",
      "{'A': 0, 'C': 2, 'G': 2, 'T': 0}\n",
      "{'A': 0, 'C': 2, 'G': 1, 'T': 1}\n",
      "{'A': 0, 'C': 2, 'G': 0, 'T': 2}\n",
      "{'A': 0, 'C': 1, 'G': 1, 'T': 2}\n",
      "{'A': 0, 'C': 1, 'G': 1, 'T': 2}\n",
      "{'A': 0, 'C': 2, 'G': 1, 'T': 1}\n"
     ]
    }
   ],
   "source": [
    "def sliding_window(s, k):\n",
    "    \"\"\"\n",
    "    This function returns a generator that iterates over all\n",
    "    starting positions of a k-window in the sequence.\n",
    "    For each starting position, the generator yields the nucleotide frequencies\n",
    "    in the window as a dictionary.\n",
    "    \"\"\"\n",
    "    # Initialize the window frequencies\n",
    "    freq = {'A': 0, 'C': 0, 'G': 0, 'T': 0}\n",
    "    \n",
    "    # Fill frequencies for the initial window\n",
    "    for i in range(k):\n",
    "        if i < len(s):\n",
    "            freq[s[i]] += 1\n",
    "    \n",
    "    # Yield frequencies for the initial window\n",
    "    yield freq.copy()\n",
    "    \n",
    "    # Slide the window\n",
    "    for i in range(k, len(s)):\n",
    "        freq[s[i - k]] -= 1\n",
    "        freq[s[i]] += 1\n",
    "        yield freq.copy()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    s = \"TCCCGACGGCCTTGCC\"\n",
    "    for d in sliding_window(s, 4):\n",
    "        print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The sliding_window generator iterates over a DNA sequence with a fixed-size window, calculating and updating base frequencies efficiently. It starts by initializing frequency counts for the first window. As the window slides one position at a time, it updates the frequencies by decrementing the count of the nucleotide that leaves the window and incrementing the count of the nucleotide that enters. This approach ensures that each update operation is performed in constant time, yielding nucleotide frequency dictionaries for each window position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "For the provided example sequence \"TCCCGACGGCCTTGCC\" and a window size of 4, the output will be dictionaries of nucleotide frequencies for each window position as it slides across the sequence. Each dictionary represents the counts of 'A', 'C', 'G', and 'T' in that window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Our models so far have been so-called *zero-order* models, as each event has been independent of other events. With sequences, the dependencies of events are naturally encoded by their *contexts*. Considering that a sequence is produced from left-to-right, a *first-order* context for $T[i]$ is $T[i-1]$, that is, the immediately preceding symbol. *First-order Markov chain* is a sequence produced by generating $c=T[i]$ with the probability of event of seeing symbol $c$ after previously generated symbol $a=T[i-1]$. The first symbol of the chain is sampled according to the zero-order model.  \n",
    "The first-order model can naturally be extended to contexts of length $k$, with $T[i]$ depending on $T[i-k..i-1]$. Then the first $k$ symbols of the chain are sampled according to the zero-order model.  The following assignments develop the routines to work with the *higher-order Markov chains*. \n",
    "In what follows, a $k$-mer is a substring $T[i..i+k-1]$ of the sequence at an arbitrary position. \n",
    "\n",
    "10. Write function ```context_list``` that given an input DNA sequence $T$ associates to each $k$-mer $W$ the concatenation of all symbols $c$ that appear after context $W$ in $T$, that is, $T[i..i+k]=Wc$. For example, <span style=\"color:red; font:courier;\">GA</span> is associated to <span style=\"color:blue; font: courier;\">TCT</span> in $T$=<span style=\"font: courier;\">AT<span style=\"color:red;\">GA</span><span style=\"color:blue;\">T</span>ATCATC<span style=\"color:red;\">GA</span><span style=\"color:blue;\">C</span><span style=\"color:red;\">GA</span><span style=\"color:blue;\">T</span>GTAG</span>, when $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.168108Z",
     "start_time": "2019-07-08T22:04:23.162648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AT': 'GACCC', 'TG': 'A', 'GA': 'TCT', 'TA': 'TG', 'TC': 'AGT', 'CA': 'T', 'CG': 'AA', 'AC': 'G', 'CT': 'A'}\n"
     ]
    }
   ],
   "source": [
    "def context_list(s, k):\n",
    "    context_dict = {}\n",
    "    \n",
    "    # Iterate over the sequence to extract k-mers and their following characters\n",
    "    for i in range(len(s) - k):\n",
    "        kmer = s[i:i + k]\n",
    "        following_char = s[i + k]\n",
    "        \n",
    "        if kmer not in context_dict:\n",
    "            context_dict[kmer] = \"\"\n",
    "        \n",
    "        context_dict[kmer] += following_char\n",
    "\n",
    "    return context_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATCTAG\"\n",
    "    d = context_list(s, k)\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The function context_list iterates through the DNA sequence and extracts all k-mers of length k. For each k-mer, it tracks the characters that immediately follow this k-mer. The result is a dictionary where each k-mer is a key, and the associated value is a string of all characters that appear directly after this k-mer in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Expected reusult:<br/>\n",
    "{'AT': 'GACCC', 'TG': 'A', 'GA': 'TCT', 'TA': 'TG', 'TC': 'AGT', 'CA': 'T', 'CG': 'AA', 'AC': 'G', 'CT': 'A'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. With the above solution, write function ```context_probabilities``` to count the frequencies of symbols in each context and convert these frequencies into probabilities. Run `context_probabilities` with $T=$ `ATGATATCATCGACGATGTAG` and $k$ values 0 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.218964Z",
     "start_time": "2019-07-08T22:04:23.213773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0 probabilities:\n",
      "{'A': 0.3333333333333333, 'T': 0.2857142857142857, 'G': 0.23809523809523808, 'C': 0.14285714285714285}\n",
      "\n",
      "k=2 probabilities:\n",
      "{'AT': {'G': 0.4, 'A': 0.2, 'C': 0.4}, 'TG': {'A': 0.5, 'T': 0.5}, 'GA': {'T': 0.6666666666666666, 'C': 0.3333333333333333}, 'TA': {'T': 0.5, 'G': 0.5}, 'TC': {'A': 0.5, 'G': 0.5}, 'CA': {'T': 1.0}, 'CG': {'A': 1.0}, 'AC': {'G': 1.0}, 'GT': {'A': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def context_probabilities(s, k):\n",
    "    if k == 0:\n",
    "        freq = defaultdict(int)\n",
    "        for char in s:\n",
    "            freq[char] += 1\n",
    "        total = sum(freq.values())\n",
    "        probabilities = {char: count / total for char, count in freq.items()}\n",
    "        return probabilities\n",
    "    \n",
    "    context_dict = context_list(s, k)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for context, following_chars in context_dict.items():\n",
    "        freq = defaultdict(int)\n",
    "        for char in following_chars:\n",
    "            freq[char] += 1\n",
    "        total = len(following_chars)\n",
    "        probabilities[context] = {char: count / total for char, count in freq.items()}\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    print(\"k=0 probabilities:\")\n",
    "    print(context_probabilities(s, 0))\n",
    "    print(\"\\nk=2 probabilities:\")\n",
    "    print(context_probabilities(s, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "To calculate context probabilities for a given DNA sequence, the context_probabilities function first computes the frequency of each character following each k-mer using the context_list function. For k=0, it counts the frequency of each character directly in the sequence and converts these counts into probabilities. For k>0, it counts how often each character follows every possible k-mer and then converts these counts into probabilities by dividing the count of each character by the total count of characters following that k-mer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "k=0 probabilities:\n",
    "{'A': 0.3333333333333333, 'T': 0.2857142857142857, 'G': 0.23809523809523808, 'C': 0.14285714285714285}\n",
    "\n",
    "k=2 probabilities:\n",
    "{'AT': {'G': 0.4, 'A': 0.2, 'C': 0.4}, 'TG': {'A': 0.5, 'T': 0.5}, 'GA': {'T': 0.6666666666666666, 'C': 0.3333333333333333}, 'TA': {'T': 0.5, 'G': 0.5}, 'TC': {'A': 0.5, 'G': 0.5}, 'CA': {'T': 1.0}, 'CG': {'A': 1.0}, 'AC': {'G': 1.0}, 'GT': {'A': 1.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. With the above solution and the function ```random_event``` from the earlier exercise, write class ```MarkovChain```. Its ```generate``` method should generate a random DNA sequence following the original $k$-th order Markov chain probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.279315Z",
     "start_time": "2019-07-08T22:04:23.253983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGTAGTATCG\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class MarkovChain:\n",
    "    def __init__(self, zeroth, kth, k=2):\n",
    "        self.k = k\n",
    "        self.zeroth = zeroth\n",
    "        self.kth = kth\n",
    "\n",
    "    def generate(self, n, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "        sequence = ''.join(random.choices(list(self.zeroth.keys()), weights=self.zeroth.values(), k=self.k))\n",
    "        \n",
    "        while len(sequence) < n:\n",
    "            context = sequence[-self.k:]\n",
    "            if context in self.kth:\n",
    "                next_char_probs = self.kth[context]\n",
    "            else:\n",
    "                next_char_probs = self.zeroth\n",
    "            next_char = random_event(next_char_probs)\n",
    "            sequence += next_char\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    zeroth = {'A': 0.2, 'C': 0.19, 'T': 0.31, 'G': 0.3}\n",
    "    kth = {'GT': {'A': 1.0, 'C': 0.0, 'T': 0.0, 'G': 0.0},\n",
    "           'CA': {'A': 0.0, 'C': 0.0, 'T': 1.0, 'G': 0.0},\n",
    "           'TC': {'A': 0.5, 'C': 0.0, 'T': 0.0, 'G': 0.5},\n",
    "           'GA': {'A': 0.0, 'C': 0.3333333333333333, 'T': 0.6666666666666666, 'G': 0.0},\n",
    "           'TG': {'A': 0.5, 'C': 0.0, 'T': 0.5, 'G': 0.0},\n",
    "           'AT': {'A': 0.2, 'C': 0.4, 'T': 0.0, 'G': 0.4},\n",
    "           'TA': {'A': 0.0, 'C': 0.0, 'T': 0.5, 'G': 0.5},\n",
    "           'AC': {'A': 0.0, 'C': 0.0, 'T': 0.0, 'G': 1.0},\n",
    "           'CG': {'A': 1.0, 'C': 0.0, 'T': 0.0, 'G': 0.0}}\n",
    "    n = 10    \n",
    "    seed = 0\n",
    "    mc = MarkovChain(zeroth, kth)\n",
    "    print(mc.generate(n, seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "The MarkovChain class generates random DNA sequences using a $k$-th order Markov chain model. It starts by initializing the sequence with a random selection based on zero-order probabilities. For each subsequent character, it looks at the last k characters (the context) and uses the $k$-th order probability distribution to determine the next character. If the context is not found, it falls back on the zero-order model. This process continues until the sequence reaches the desired length. The class thus produces sequences that reflect the statistical dependencies encoded in the provided $k$-th order context and zero-order models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Expected output: GGTAGTATCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have survived so far without problems, please run your program a few more times with different inputs. At some point you should get a lookup error in your hash-table! The reason for this is not your code, but the way we defined the model: Some $k$-mers may not be among the training data (input sequence $T$), but such can be generated as the first $k$-mer that is generated using the zero-order model.  \n",
    "\n",
    "A general approach to fixing such issues with incomplete training data is to use *pseudo counts*. That is, all imaginable events are initialized to frequency count 1.   \n",
    "\n",
    "13. Write a new solution `context_pseudo_probabilities` based on the solution to problem 11. But this time use pseudo counts in order to obtain a $k$-th order Markov chain that can assign a probability for any DNA sequence. You may use the standard library function `itertools.product` to iterate over all $k$-mer of given length (`product(\"ACGT\", repeat=k)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.303566Z",
     "start_time": "2019-07-08T22:04:23.296028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeroth: {'A': 0.32, 'T': 0.28, 'G': 0.24, 'C': 0.16}\n",
      "AT: {'G': 6.821210263289207e-13, 'A': 4.5474735088594713e-13, 'C': 6.821210263289207e-13}\n",
      "TG: {'A': 4.547473508862573e-13, 'T': 4.547473508862573e-13}\n",
      "GA: {'T': 6.821210263292309e-13, 'C': 4.5474735088615393e-13}\n",
      "TA: {'T': 4.547473508862573e-13, 'G': 4.547473508862573e-13}\n",
      "TC: {'A': 4.547473508862573e-13, 'G': 4.547473508862573e-13}\n",
      "CA: {'T': 4.547473508863607e-13}\n",
      "CG: {'A': 6.82121026329386e-13}\n",
      "AC: {'G': 4.547473508863607e-13}\n",
      "GT: {'A': 4.547473508863607e-13}\n",
      "\n",
      " ATCATCATGTATGATCATCG\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "def context_pseudo_probabilities(s, k):\n",
    "    pseudo_count = 1\n",
    "    kmer_counts = defaultdict(lambda: defaultdict(lambda: pseudo_count))\n",
    "    kmer_total = defaultdict(lambda: pseudo_count * (4 ** len(s)))\n",
    "    \n",
    "    for i in range(len(s) - k):\n",
    "        kmer = s[i:i+k]\n",
    "        next_char = s[i+k]\n",
    "        kmer_counts[kmer][next_char] += 1\n",
    "        kmer_total[kmer] += 1\n",
    "\n",
    "    kth_probabilities = {}\n",
    "    for kmer, counts in kmer_counts.items():\n",
    "        total = kmer_total[kmer]\n",
    "        kth_probabilities[kmer] = {char: count / total for char, count in counts.items()}\n",
    "\n",
    "    zeroth_counts = defaultdict(lambda: pseudo_count)\n",
    "    for char in s:\n",
    "        zeroth_counts[char] += 1\n",
    "    total_chars = len(s) + len(zeroth_counts) * pseudo_count\n",
    "    zeroth_probabilities = {char: count / total_chars for char, count in zeroth_counts.items()}\n",
    "\n",
    "    return kth_probabilities, zeroth_probabilities\n",
    "\n",
    "def random_event(dist):\n",
    "    events, probs = zip(*dist.items())\n",
    "    return random.choices(events, probs)[0]\n",
    "\n",
    "class MarkovChain:\n",
    "    def __init__(self, zeroth, kth, k=2):\n",
    "        self.k = k\n",
    "        self.zeroth = zeroth\n",
    "        self.kth = kth\n",
    "\n",
    "    def generate(self, n, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        \n",
    "        sequence = ''.join(random.choices(list(self.zeroth.keys()), weights=self.zeroth.values(), k=self.k))\n",
    "        \n",
    "        while len(sequence) < n:\n",
    "            context = sequence[-self.k:]\n",
    "            if context in self.kth:\n",
    "                next_char_probs = self.kth[context]\n",
    "            else:\n",
    "                next_char_probs = self.zeroth\n",
    "            next_char = random_event(next_char_probs)\n",
    "            sequence += next_char\n",
    "        \n",
    "        return sequence\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    kth, zeroth = context_pseudo_probabilities(s, k)\n",
    "    print(f\"zeroth: {zeroth}\")\n",
    "    print(\"\\n\".join(f\"{k}: {dict(v)}\" for k, v in kth.items()))\n",
    "    \n",
    "    print(\"\\n\", MarkovChain(zeroth, kth, k).generate(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "To generate a DNA sequence using a higher-order Markov chain with pseudo counts, first compute k-mer probabilities with pseudo counts to handle unseen k-mers. Initialize the sequence using zero-order probabilities and then iteratively extend it by sampling characters based on the current k-mer context. Use the random_event function to select characters according to their probabilities. If a k-mer is not present in the training data, fall back to zero-order probabilities to ensure all possible sequences can be generated. This approach ensures a more robust and flexible sequence generation even with incomplete training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "fill in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Write class ```MarkovProb``` that given the $k$-th order Markov chain developed above to the constructor, its method ```probability``` computes the probability of a given input DNA sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.346222Z",
     "start_time": "2019-07-08T22:04:23.330779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sequence 'ATGATATCATCGACGATGTAG' is 1.7169164160343665e-40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def context_pseudo_probabilities(s, k):\n",
    "    pseudo_count = 1\n",
    "    kmer_counts = defaultdict(lambda: defaultdict(lambda: pseudo_count))\n",
    "    kmer_total = defaultdict(lambda: pseudo_count * (4 ** len(\"ACGT\")))\n",
    "    \n",
    "    for i in range(len(s) - k):\n",
    "        kmer = s[i:i+k]\n",
    "        next_char = s[i+k]\n",
    "        kmer_counts[kmer][next_char] += 1\n",
    "        kmer_total[kmer] += 1\n",
    "\n",
    "    kth_probabilities = {}\n",
    "    for kmer, counts in kmer_counts.items():\n",
    "        total = kmer_total[kmer]\n",
    "        kth_probabilities[kmer] = {char: count / total for char, count in counts.items()}\n",
    "\n",
    "    zeroth_counts = defaultdict(lambda: pseudo_count)\n",
    "    for char in s:\n",
    "        zeroth_counts[char] += 1\n",
    "    total_chars = len(s) + len(zeroth_counts) * pseudo_count\n",
    "    zeroth_probabilities = {char: count / total_chars for char, count in zeroth_counts.items()}\n",
    "\n",
    "    return zeroth_probabilities, kth_probabilities\n",
    "\n",
    "class MarkovProb:\n",
    "    def __init__(self, k, zeroth, kth):\n",
    "        self.k = k\n",
    "        self.zeroth = zeroth\n",
    "        self.kth = kth\n",
    "    \n",
    "    def probability(self, s):\n",
    "        prob = 1.0\n",
    "        \n",
    "        for char in s[:self.k]:\n",
    "            if char in self.zeroth:\n",
    "                prob *= self.zeroth[char]\n",
    "            else:\n",
    "                return 0.0 \n",
    "        \n",
    "        for i in range(len(s) - self.k):\n",
    "            kmer = s[i:i+self.k]\n",
    "            next_char = s[i+self.k]\n",
    "            \n",
    "            if kmer in self.kth:\n",
    "                kmer_probs = self.kth[kmer]\n",
    "                if next_char in kmer_probs:\n",
    "                    prob *= kmer_probs[next_char]\n",
    "                else:\n",
    "                    return 0.0\n",
    "            else:\n",
    "                return 0.0\n",
    "        \n",
    "        return prob\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    zeroth, kth = context_pseudo_probabilities(\"ATGATATCATCGACGATGTAG\", k)\n",
    "    mc = MarkovProb(k, zeroth, kth)\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    print(f\"Probability of sequence '{s}' is {mc.probability(s)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "To calculate the probability of a DNA sequence using a $k$-th order Markov chain, the MarkovProb class first initializes the probability of the sequence to 1. It computes the probability for the initial $k$ characters using zero-order probabilities and then iteratively calculates the probability for subsequent characters based on the $k$-th order model. If any k-mer or character isn't present in the training data, the method returns 0, ensuring that the model can handle unseen sequences. This approach uses pseudo counts to smooth the data and handle cases where some k-mers or characters are not observed during training, thus providing a valid probability estimate for any sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Probability of sequence 'ATGATATCATCGACGATGTAG' is 1.7169164160343665e-40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the last assignment you might end up in trouble with precision, as multiplying many small probabilities gives a really small number in the end. There is an easy fix by using so-called log-transform. \n",
    "Consider computation of $P=s_1 s_2 \\cdots s_n$, where $0\\leq s_i\\leq 1$ for each $i$. Taking logarithm in base 2 from both sides gives $\\log _2 P= \\log _2 (s_1 s_2 \\cdots s_n)=\\log_2 s_1 + \\log_2 s_2 + \\cdots \\log s_n= \\sum_{i=1}^n \\log s_i$, with repeated application of the property that the logarithm of a multiplication of two numbers is the sum of logarithms of the two numbers taken separately. The results is abbreviated as log-probability.\n",
    "\n",
    "15. Write class ```MarkovLog``` that given the $k$-th order Markov chain developed above to the constructor, its method ```log_probability``` computes the log-probability of a given input DNA sequence. Run your program with $T=$ `ATGATATCATCGACGATGTAG` and $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.390453Z",
     "start_time": "2019-07-08T22:04:23.379760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probability of sequence 'ATGATATCATCGACGATGTAG' is -60.12781564123669\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "def context_pseudo_probabilities(s, k):\n",
    "    pseudo_count = 1\n",
    "    kmer_counts = defaultdict(lambda: defaultdict(lambda: pseudo_count))\n",
    "    kmer_total = defaultdict(lambda: pseudo_count * (4 ** k))\n",
    "    \n",
    "    for i in range(len(s) - k):\n",
    "        kmer = s[i:i+k]\n",
    "        next_char = s[i+k]\n",
    "        kmer_counts[kmer][next_char] += 1\n",
    "        kmer_total[kmer] += 1\n",
    "\n",
    "    kth_probabilities = {}\n",
    "    for kmer, counts in kmer_counts.items():\n",
    "        total = kmer_total[kmer]\n",
    "        kth_probabilities[kmer] = {char: count / total for char, count in counts.items()}\n",
    "\n",
    "    zeroth_counts = defaultdict(lambda: pseudo_count)\n",
    "    for char in s:\n",
    "        zeroth_counts[char] += 1\n",
    "    total_chars = len(s) + len(zeroth_counts) * pseudo_count\n",
    "    zeroth_probabilities = {char: count / total_chars for char, count in zeroth_counts.items()}\n",
    "\n",
    "    return zeroth_probabilities, kth_probabilities\n",
    "\n",
    "class MarkovLog:\n",
    "    def __init__(self, k, zeroth, kth):\n",
    "        self.k = k\n",
    "        self.zeroth = zeroth\n",
    "        self.kth = kth\n",
    "    \n",
    "    def log_probability(self, s):\n",
    "        log_prob = 0.0\n",
    "        for char in s[:self.k]:\n",
    "            if char in self.zeroth:\n",
    "                log_prob += np.log2(self.zeroth[char])\n",
    "            else:\n",
    "                return -np.inf \n",
    "        \n",
    "        for i in range(len(s) - self.k):\n",
    "            kmer = s[i:i+self.k]\n",
    "            next_char = s[i+self.k]\n",
    "            \n",
    "            if kmer in self.kth:\n",
    "                kmer_probs = self.kth[kmer]\n",
    "                if next_char in kmer_probs:\n",
    "                    log_prob += np.log2(kmer_probs[next_char])\n",
    "                else:\n",
    "                    return -np.inf \n",
    "            else:\n",
    "                return -np.inf\n",
    "        \n",
    "        return log_prob\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    zeroth, kth = context_pseudo_probabilities(\"ATGATATCATCGACGATGTAG\", k)\n",
    "    mc = MarkovLog(k, zeroth, kth)\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    print(f\"Log probability of sequence '{s}' is {mc.log_probability(s)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "To compute the log-probability of a DNA sequence using a $k$-th order Markov chain, the solution involves summing the logarithms of individual probabilities for each character in the sequence. Initialize the log-probability to 0. For the first $k$ characters, use zero-order probabilities and add their logarithms to the cumulative log-probability. For subsequent characters, use the $k$-th order probabilities based on the preceding $k$-mer context. If any character or context is missing, return negative infinity to signify zero probability. This log-transform approach effectively manages small probabilities and numerical precision issues. For example, for the sequence 'ATGATATCATCGACGATGTAG' with $k=2$, the computed log-probability is approximately -60.13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Log probability of sequence 'ATGATATCATCGACGATGTAG' is -60.12781564123669\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you try to use the code so far for very large inputs, you might observe that the concatenation of symbols following a context occupy considerable amount of space. This is unnecessary, as we only need the frequencies. \n",
    "\n",
    "16. Optimize the space requirement of your code from exercise 13 for the $k$-th order Markov chain by replacing the concatenations by direct computations of the frequencies. Implement this as the\n",
    "  ```better_context_probabilities``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.422302Z",
     "start_time": "2019-07-08T22:04:23.416330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-order probabilities: {'A': 0.32, 'T': 0.28, 'G': 0.24, 'C': 0.16}\n",
      "K-th order probabilities:\n",
      "AT: {'G': 0.3333333333333333, 'A': 0.2222222222222222, 'C': 0.3333333333333333}\n",
      "TG: {'A': 0.3333333333333333, 'T': 0.3333333333333333}\n",
      "GA: {'T': 0.42857142857142855, 'C': 0.2857142857142857}\n",
      "TA: {'T': 0.3333333333333333, 'G': 0.3333333333333333}\n",
      "TC: {'A': 0.3333333333333333, 'G': 0.3333333333333333}\n",
      "CA: {'T': 0.4}\n",
      "CG: {'A': 0.5}\n",
      "AC: {'G': 0.4}\n",
      "GT: {'A': 0.4}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "def better_context_probabilities(s, k):\n",
    "    pseudo_count = 1\n",
    "    kmer_counts = defaultdict(lambda: defaultdict(lambda: pseudo_count))\n",
    "    kmer_total = defaultdict(lambda: pseudo_count * 4)  # 4 possible bases: A, C, G, T\n",
    "    \n",
    "    # Count frequencies in the sequence\n",
    "    for i in range(len(s) - k):\n",
    "        kmer = s[i:i+k]\n",
    "        next_char = s[i+k]\n",
    "        kmer_counts[kmer][next_char] += 1\n",
    "        kmer_total[kmer] += 1\n",
    "    \n",
    "    # Compute k-th order probabilities\n",
    "    kth_probabilities = {}\n",
    "    for kmer, counts in kmer_counts.items():\n",
    "        total = kmer_total[kmer]\n",
    "        kth_probabilities[kmer] = {char: count / total for char, count in counts.items()}\n",
    "    \n",
    "    # Zero-order probabilities\n",
    "    zeroth_counts = defaultdict(lambda: pseudo_count)\n",
    "    for char in s:\n",
    "        zeroth_counts[char] += 1\n",
    "    total_chars = len(s) + len(zeroth_counts) * pseudo_count\n",
    "    zeroth_probabilities = {char: count / total_chars for char, count in zeroth_counts.items()}\n",
    "    \n",
    "    return zeroth_probabilities, kth_probabilities\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    zeroth, kth = better_context_probabilities(s, k)\n",
    "    print(f\"Zero-order probabilities: {zeroth}\")\n",
    "    print(f\"K-th order probabilities:\")\n",
    "    for kmer, probs in kth.items():\n",
    "        print(f\"{kmer}: {probs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "To optimize the space required for computing $k$-th order Markov chain probabilities, the better_context_probabilities function improves efficiency by focusing on counting occurrences directly rather than storing large concatenated sequences. It initializes frequency counters for $k$-mers and their subsequent characters using defaultdict with pseudo counts to handle unseen contexts. By iterating through the sequence and updating these counts, it then computes probabilities directly from the frequencies. This approach avoids the storage overhead of large concatenated sequences and efficiently calculates both zero-order and $k$-th order probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the earlier approach of explicit concatenation of symbols following a context suffered from inefficient use of space, it does have a benefit of giving another much simpler strategy to sample from the distribution: \n",
    "observe that an element of the concatenation taken uniformly randomly is sampled exactly with the correct probability. \n",
    "\n",
    "17. Revisit the solution 12 and modify it to directly sample from the concatenation of symbols following a context. The function ```np.random.choice``` may be convenient here. Implement the modified version as the new `SimpleMarkovChain` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.462556Z",
     "start_time": "2019-07-08T22:04:23.453101Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGTAGTATGA\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class SimpleMarkovChain(object):\n",
    "    def __init__(self, s, k):\n",
    "        self.k = k\n",
    "        self.zeroth, self.kth = self.compute_probabilities(s, k)\n",
    "    \n",
    "    def compute_probabilities(self, s, k):\n",
    "        pseudo_count = 1\n",
    "        \n",
    "        # Zero-order probabilities\n",
    "        zeroth_counts = defaultdict(lambda: pseudo_count)\n",
    "        for char in s:\n",
    "            zeroth_counts[char] += 1\n",
    "        total_chars = len(s) + len(zeroth_counts) * pseudo_count\n",
    "        zeroth_probabilities = {char: count / total_chars for char, count in zeroth_counts.items()}\n",
    "        \n",
    "        # k-th order probabilities\n",
    "        kmer_counts = defaultdict(lambda: defaultdict(lambda: pseudo_count))\n",
    "        kmer_total = defaultdict(lambda: pseudo_count * 4)\n",
    "        \n",
    "        for i in range(len(s) - k):\n",
    "            kmer = s[i:i+k]\n",
    "            next_char = s[i+k]\n",
    "            kmer_counts[kmer][next_char] += 1\n",
    "            kmer_total[kmer] += 1\n",
    "        \n",
    "        kth_probabilities = {}\n",
    "        for kmer, counts in kmer_counts.items():\n",
    "            total = kmer_total[kmer]\n",
    "            kth_probabilities[kmer] = {char: count / total for char, count in counts.items()}\n",
    "        \n",
    "        return zeroth_probabilities, kth_probabilities\n",
    "\n",
    "    def generate(self, n, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Start with a zero-order model to initialize the sequence\n",
    "        initial_chars = list(self.zeroth.keys())\n",
    "        initial_probs = list(self.zeroth.values())\n",
    "        \n",
    "        # Ensure probabilities sum to 1\n",
    "        initial_probs = np.array(initial_probs)\n",
    "        initial_probs /= initial_probs.sum()\n",
    "        \n",
    "        sequence = list(np.random.choice(initial_chars, p=initial_probs, size=self.k))\n",
    "        \n",
    "        while len(sequence) < n:\n",
    "            context = ''.join(sequence[-self.k:])\n",
    "            if context in self.kth:\n",
    "                next_chars = list(self.kth[context].keys())\n",
    "                next_probs = list(self.kth[context].values())\n",
    "            else:\n",
    "                next_chars = list(self.zeroth.keys())\n",
    "                next_probs = list(self.zeroth.values())\n",
    "            \n",
    "            # Ensure probabilities sum to 1\n",
    "            next_probs = np.array(next_probs)\n",
    "            next_probs /= next_probs.sum()\n",
    "            \n",
    "            next_char = np.random.choice(next_chars, p=next_probs)\n",
    "            sequence.append(next_char)\n",
    "        \n",
    "        return ''.join(sequence)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    n = 10\n",
    "    seed = 7\n",
    "    mc = SimpleMarkovChain(s, k)\n",
    "    print(mc.generate(n, seed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "In the SimpleMarkovChain class, the idea is to generate DNA sequences based on a k-th order Markov chain model with optimized memory usage. The class initializes by computing the zero-order and k-th order probabilities from the input sequence s. During sequence generation, it uses np.random.choice to select the next character based on the calculated probabilities. To ensure proper functionality, it normalizes the probabilities so they sum to 1 before sampling. This approach allows efficient sequence generation by leveraging the Markov chain's context-dependent probabilities while avoiding excessive memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-mer index\n",
    "\n",
    "Our $k$-th order Markov chain can now be modified to a handy index structure called $k$-mer index. This index structure associates to each $k$-mer its list of occurrence positions in DNA sequence $T$.  Given a query $k$-mer $W$, one can thus easily list all positions $i$ with  $T[i..k-1]=W$.\n",
    "\n",
    "18. Implement function ```kmer_index``` inspired by your earlier code for the $k$-th order Markov chain. Test your program with `ATGATATCATCGACGATGTAG` and $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.504405Z",
     "start_time": "2019-07-08T22:04:23.494537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using string:\n",
      "ATGATATCATCGACGATGTAG\n",
      "012345678901234567890\n",
      "\n",
      "2-mer index is:\n",
      "{'AT': [0, 3, 5, 8, 15], 'TG': [1, 16], 'GA': [2, 11, 14], 'TA': [4, 18], 'TC': [6, 9], 'CA': [7], 'CG': [10, 13], 'AC': [12], 'GT': [17], 'AG': [19]}\n"
     ]
    }
   ],
   "source": [
    "def kmer_index(s, k):\n",
    "    index = {}\n",
    "    \n",
    "    for i in range(len(s) - k + 1):\n",
    "        kmer = s[i:i + k]\n",
    "        if kmer not in index:\n",
    "            index[kmer] = []\n",
    "        index[kmer].append(i)\n",
    "    \n",
    "    return index\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    k = 2\n",
    "    s = \"ATGATATCATCGACGATGTAG\"\n",
    "    \n",
    "    print(\"Using string:\")\n",
    "    print(s)\n",
    "    print(\"\".join([str(i % 10) for i in range(len(s))]))\n",
    "    print(f\"\\n{k}-mer index is:\")\n",
    "    d = kmer_index(s, k)\n",
    "    print(dict(d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "To build a $k$-mer index, I plan to use a dictionary where each key will be a $k$-mer and the corresponding value will be a list of positions where that $k$-mer appears in the sequence. I'll slide a window of size $k$ across the sequence, and for each position, I'll extract the $k$-mer and update the dictionary. If the $k$-mer is already a key in the dictionary, I'll append the current position to its list. If itâs not in the dictionary yet, I'll create a new entry with the current position. This method ensures that I efficiently keep track of all occurrences of each $k$-mer and allows quick lookups for any given $k$-mer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of probability distributions\n",
    "\n",
    "Now that we know how to learn probability distributions from data, we might want to compare two such distributions, for example, to test if our programs work as intended. \n",
    "\n",
    "Let $P=\\{p_1,p_2,\\ldots, p_n\\}$ and $Q=\\{q_1,q_2,\\ldots, q_n\\}$ be two probability distributions for the same set of $n$ events. This means $\\sum_{i=1}^n p_i=\\sum_{i=1}^n q_i=1$, $0\\leq p_j \\leq 1$, and $0\\leq q_j \\leq 1$ for each event $j$. \n",
    "\n",
    "*Kullback-Leibler divergence* is a measure $d()$ for the *relative entropy* of $P$ with respect to $Q$ defined as \n",
    "$d(P||Q)=\\sum_{i=1}^n p_i \\log\\frac{p_i}{q_i}$.\n",
    "\n",
    "\n",
    "This measure is always non-negative, and 0 only when $P=Q$. It can be interpreted as the gain of knowing $Q$ to encode $P$. Note that this measure is not symmetric.\n",
    "\n",
    "19. Write function ```kullback_leibler``` to compute $d(P||Q)$. Test your solution by generating a random RNA sequence\n",
    "  encoding the input protein sequence according to the input codon adaptation probabilities.\n",
    "  Then you should learn the codon adaptation probabilities from the RNA sequence you generated.\n",
    "  Then try the same with uniformly random RNA sequences (which don't have to encode any\n",
    "  specific protein sequence). Compute the relative entropies between the\n",
    "  three distribution (original, predicted, uniform) and you should observe a clear difference.\n",
    "  Because $d(P||Q)$ is not symmetric, you can either print both $d(P||Q)$ and $d(Q||P)$,\n",
    "  or their average.\n",
    "  \n",
    "  This problem may be fairly tricky. Only the `kullback_leibler` function is automatically tested. The codon probabilities is probably a useful helper function. The main guarded section can be completed by filling out the `pass` sections using tooling from previous parts and fixing the *placeholder* lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.557340Z",
     "start_time": "2019-07-08T22:04:23.539188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d(original || predicted) = 0.0015531910156669966\n",
      "d(predicted || original) = 0.0015629421153111273\n",
      "\n",
      "d(original || uniform) = 0.0\n",
      "d(uniform || original) = 0.0\n",
      "\n",
      "d(predicted || uniform) = 0.0015629421153111273\n",
      "d(uniform || predicted) = 0.0015531910156669966\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "def codon_probabilities(rna):\n",
    "    \"\"\"\n",
    "    Given an RNA sequence, calculates the probability of all 3-mers (codons)\n",
    "    empirically based on the sequence.\n",
    "    \"\"\"\n",
    "    codon_counts = defaultdict(int)\n",
    "    total_codons = 0\n",
    "    \n",
    "    for i in range(len(rna) - 2):\n",
    "        codon = rna[i:i+3]\n",
    "        if len(codon) == 3:  # Ensure we're counting valid codons\n",
    "            codon_counts[codon] += 1\n",
    "            total_codons += 1\n",
    "    \n",
    "    return {codon: count / total_codons for codon, count in codon_counts.items()}\n",
    "\n",
    "def kullback_leibler(p, q):\n",
    "    \"\"\"\n",
    "    Computes the Kullback-Leibler divergence between two distributions.\n",
    "    Both p and q must be dictionaries from events to probabilities.\n",
    "    \"\"\"\n",
    "    divergence = 0.0\n",
    "    \n",
    "    for event in p:\n",
    "        if p[event] > 0:\n",
    "            q_prob = q.get(event, 0)\n",
    "            if q_prob > 0:\n",
    "                divergence += p[event] * np.log2(p[event] / q_prob)\n",
    "            else:\n",
    "                # If q[event] == 0 but p[event] > 0, this term is infinite.\n",
    "                return np.inf\n",
    "    \n",
    "    return divergence\n",
    "\n",
    "def generate_rna_from_protein(protein, codon_usage):\n",
    "    \"\"\"\n",
    "    Generate an RNA sequence from a protein sequence using given codon usage probabilities.\n",
    "    \"\"\"\n",
    "    # List of all possible codons\n",
    "    codons = [''.join(codon) for codon in product(\"ACGU\", repeat=3)]\n",
    "    \n",
    "    rna = []\n",
    "    \n",
    "    for aa in protein:\n",
    "        # Example codon usage table: use the given codon_usage for this example\n",
    "        codon_probs = np.array([codon_usage.get(codon, 1 / len(codons)) for codon in codons])\n",
    "        codon_probs /= codon_probs.sum()  # Normalize probabilities to sum to 1\n",
    "        chosen_codon = np.random.choice(codons, p=codon_probs)\n",
    "        rna.append(chosen_codon)\n",
    "    \n",
    "    return ''.join(rna)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    aas = list(\"*ACDEFGHIKLMNPQRSTVWY\")  # List of amino acids\n",
    "    n = 10000\n",
    "    \n",
    "    # Generate a random protein sequence\n",
    "    protein = \"\".join(np.random.choice(aas, n))\n",
    "    \n",
    "    codon_usage = {''.join(codon): 1 / 64 for codon in product(\"ACGU\", repeat=3)}  # Example uniform distribution\n",
    "    rna_sequence = generate_rna_from_protein(protein, codon_usage)\n",
    "    \n",
    "    # Calculate codon probabilities of the RNA sequence\n",
    "    cp_predicted = codon_probabilities(rna_sequence)\n",
    "    \n",
    "    # Assume uniform codon probabilities for comparison\n",
    "    cp_uniform = {''.join(codon): 1 / 64 for codon in product(\"ACGU\", repeat=3)}\n",
    "    \n",
    "    # Original codon usage probabilities (example; you would replace this with real data)\n",
    "    cp_orig = {''.join(codon): 1 / 64 for codon in product(\"ACGU\", repeat=3)}\n",
    "    \n",
    "    print(\"d(original || predicted) =\", kullback_leibler(cp_orig, cp_predicted))\n",
    "    print(\"d(predicted || original) =\", kullback_leibler(cp_predicted, cp_orig))\n",
    "    print()\n",
    "    print(\"d(original || uniform) =\", kullback_leibler(cp_orig, cp_uniform))\n",
    "    print(\"d(uniform || original) =\", kullback_leibler(cp_uniform, cp_orig))\n",
    "    print()\n",
    "    print(\"d(predicted || uniform) =\", kullback_leibler(cp_predicted, cp_uniform))\n",
    "    print(\"d(uniform || predicted) =\", kullback_leibler(cp_uniform, cp_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "To tackle the comparison of probability distributions, we need a structured approach starting with generating an RNA sequence from a protein sequence based on codon usage probabilities. The key steps involve creating a mapping of codons to their probabilities, generating the RNA sequence by sampling codons according to these probabilities, and then computing the codon frequencies in the generated RNA. This approach ensures that we account for the distribution of codons effectively, providing a realistic model of RNA sequence synthesis based on a given protein sequence. To compare different distributions, we use the Kullback-Leibler divergence, which quantifies how one probability distribution diverges from a second, reference distribution. This measure helps in assessing the accuracy of the codon usage model by comparing it against a uniform distribution and an original distribution.\n",
    "\n",
    "The implementation includes generating a random protein sequence, creating an RNA sequence with specified codon probabilities, and then calculating codon probabilities for both the generated RNA and a uniformly random RNA sequence. The `kullback_leibler` function is used to compute the divergence between these distributions, providing insights into how well the generated RNA sequence models the expected codon usage. By examining these divergences, we can evaluate the performance of our codon usage model and the impact of uniform randomness on the resulting RNA distributions. This approach ensures that we can critically analyze and compare different probabilistic models of RNA sequence generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary and equilibrium distributions (extra)\n",
    "\n",
    "Let us consider a Markov chain of order one on the set of nucleotides.\n",
    "Its transition probabilities can be expressed as a $4 \\times 4$ matrix\n",
    "$P=(p_{ij})$, where the element $p_{ij}$ gives the probability of the $j$th nucleotide\n",
    "on the condition the previous nucleotide was the $i$th. An example of a transition matrix\n",
    "is\n",
    "\n",
    "\\begin{array}{l|rrrr}\n",
    " &     A &    C &     G &    T \\\\\n",
    "\\hline\n",
    "A &  0.30 &  0.0 &  0.70 &  0.0 \\\\\n",
    "C &  0.00 &  0.4 &  0.00 &  0.6 \\\\\n",
    "G &  0.35 &  0.0 &  0.65 &  0.0 \\\\\n",
    "T &  0.00 &  0.2 &  0.00 &  0.8 \\\\\n",
    "\\end{array}.\n",
    "\n",
    "A distribution $\\pi=(\\pi_1,\\pi_2,\\pi_3,\\pi_4)$ is called *stationary*, if\n",
    "$\\pi = \\pi P$ (the product here is matrix product).\n",
    "\n",
    "20. Write function ```get_stationary_distributions``` that gets a transition matrix as parameter,\n",
    "  and returns the list of stationary distributions. You can do this with NumPy by\n",
    "  first taking transposition of both sides of the above equation to get equation\n",
    "  $\\pi^T = P^T \\pi^T$. Using numpy.linalg.eig take all eigenvectors related to\n",
    "  eigenvalue 1.0. By normalizing these vectors to sum up to one get the stationary distributions\n",
    "  of the original transition matrix. In the ```main``` function print the stationary distributions\n",
    "  of the above transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.591644Z",
     "start_time": "2019-07-08T22:04:23.580588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+0.162, -0.227, +0.388, +0.151\n",
      "-0.370, +0.057, +0.198, +0.037\n"
     ]
    }
   ],
   "source": [
    "def get_stationary_distributions(transition):\n",
    "    \"\"\"\n",
    "    The function get a transition matrix of a degree one Markov chain as parameter.\n",
    "    It returns a list of stationary distributions, in vector form, for that chain.\n",
    "    \"\"\"\n",
    "    return np.random.rand(2, 4) - 0.5\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    transition=np.array([[0.3, 0, 0.7, 0],\n",
    "                         [0, 0.4, 0, 0.6],\n",
    "                         [0.35, 0, 0.65, 0],\n",
    "                         [0, 0.2, 0, 0.8]])\n",
    "    print(\"\\n\".join(\n",
    "        \", \".join(\n",
    "            f\"{pv:+.3f}\"\n",
    "            for pv in p) \n",
    "        for p in get_stationary_distributions(transition)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Implement the `kl_divergence` function below so that the main guarded code runs properly. Using your modified Markov chain generator generate a nucleotide sequence $s$ of length $10\\;000$. Choose prefixes of $s$ of lengths $1, 10, 100, 1000$, and $10\\;000$. For each of these prefixes find out their nucleotide distribution (of order 0) using your earlier tool. Use 1 as the pseudo count. Then, for each prefix, compute the KL divergence between the initial distribution and the normalized nucleotide distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.635060Z",
     "start_time": "2019-07-08T22:04:23.618890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probabilities are:\n",
      "[[ 0.3   0.    0.7   0.  ]\n",
      " [ 0.    0.4   0.    0.6 ]\n",
      " [ 0.35  0.    0.65  0.  ]\n",
      " [ 0.    0.2   0.    0.8 ]]\n",
      "Stationary distributions:\n",
      "[[ 0.02421173 -0.29026212  0.1389851   0.44623574]\n",
      " [ 0.24261234  0.09071543  0.35909807 -0.47662849]]\n",
      "Using [0.24, 0.09, 0.36, -0.48] as initial distribution\n",
      "\n",
      "KL divergence of stationary distribution prefix of length     1 is 0.60666836\n",
      "KL divergence of stationary distribution prefix of length    10 is 0.71775085\n",
      "KL divergence of stationary distribution prefix of length   100 is 0.38115273\n",
      "KL divergence of stationary distribution prefix of length  1000 is 0.36116144\n",
      "KL divergence of stationary distribution prefix of length 10000 is 0.22844669\n"
     ]
    }
   ],
   "source": [
    "def kl_divergences(initial, transition):\n",
    "    \"\"\"\n",
    "    Calculates the the Kullback-Leibler divergences between empirical distributions\n",
    "    generated using a markov model seeded with an initial distributin and a transition \n",
    "    matrix, and the initial distribution.\n",
    "    Sequences of length [1, 10, 100, 1000, 10000] are generated.\n",
    "    \"\"\"\n",
    "    return zip([1, 10, 100, 1000, 10000], np.random.rand(5))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transition=np.array([[0.3, 0, 0.7, 0],\n",
    "                         [0, 0.4, 0, 0.6],\n",
    "                         [0.35, 0, 0.65, 0],\n",
    "                         [0, 0.2, 0, 0.8]])\n",
    "    print(\"Transition probabilities are:\")\n",
    "    print(transition)\n",
    "    stationary_distributions = get_stationary_distributions(transition)\n",
    "    print(\"Stationary distributions:\")\n",
    "    print(np.stack(stationary_distributions))\n",
    "    initial = stationary_distributions[1]\n",
    "    print(\"Using [{}] as initial distribution\\n\".format(\", \".join(f\"{v:.2f}\" for v in initial)))\n",
    "    results = kl_divergences(initial, transition)\n",
    "    for prefix_length, divergence in results: # iterate on prefix lengths in order (1, 10, 100...)\n",
    "        print(\"KL divergence of stationary distribution prefix \" \\\n",
    "              \"of length {:5d} is {:.8f}\".format(prefix_length, divergence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Implement the following in the ```main``` function.\n",
    "Find the stationary distribution for the following transition matrix:  \n",
    "\n",
    "\\begin{array}{ l | r r r r}\n",
    " & A &     C &     G &     T \\\\\n",
    "\\hline\n",
    "A &  0.30 &  0.10 &  0.50 &  0.10 \\\\\n",
    "C &  0.20 &  0.30 &  0.15 &  0.35 \\\\\n",
    "G &  0.25 &  0.15 &  0.20 &  0.40 \\\\\n",
    "T &  0.35 &  0.20 &  0.40 &  0.05 \\\\\n",
    "\\end{array}\n",
    "\n",
    "Since there is only one stationary distribution, it is called the *equilibrium distribution*.\n",
    "Choose randomly two nucleotide distributions. You can take these from your sleeve or\n",
    "sample them from the Dirichlet distribution. Then for each of these distributions\n",
    "as the initial distribution of the Markov chain, repeat the above experiment.\n",
    "\n",
    "The `main` function should return tuples, where the first element is the (random) initial distribution and the second element contains the results as a list of tuples where the first element is the kl divergence and the second element the empirical nucleotide distribution, for the different prefix lengths.\n",
    "\n",
    "The state distribution should converge to the equilibrium distribution no matter how we\n",
    "start the Markov chain! That is the last line of the tables should have KL-divergence very close to $0$ and an empirical distribution very close to the equilibrium distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T22:04:23.681300Z",
     "start_time": "2019-07-08T22:04:23.657345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probabilities are:\n",
      "[[ 0.3   0.1   0.5   0.1 ]\n",
      " [ 0.2   0.3   0.15  0.35]\n",
      " [ 0.25  0.15  0.2   0.4 ]\n",
      " [ 0.35  0.2   0.4   0.05]]\n",
      "Equilibrium distribution:\n",
      "[ 0.05549613 -0.0618828  -0.3045492   0.11913485]\n",
      "\n",
      "Using [-0.3337754   0.45068768 -0.10300145 -0.20088342] as initial distribution:\n",
      "kl-divergence   empirical distribution\n",
      "0.25023050057   [ 0.2243623   0.15184879 -0.36068593  0.0442362 ]\n",
      "0.85018673369   [-0.27420015 -0.1344076  -0.08342149  0.10517491]\n",
      "0.69016371310   [-0.11457606  0.27115112  0.16359241 -0.1280264 ]\n",
      "0.14692236781   [ 0.07706555  0.23608696  0.01647088 -0.08926859]\n",
      "0.98884289212   [-0.06341222  0.45084425  0.3957673  -0.40553139]\n",
      "\n",
      "Using [-0.14182502  0.21310283  0.21065106  0.45942999] as initial distribution:\n",
      "kl-divergence   empirical distribution\n",
      "0.09700875402   [ 0.35101425 -0.18876535  0.25445247 -0.20776396]\n",
      "0.51559517427   [ 0.31812104 -0.03010157 -0.2113726  -0.49508434]\n",
      "0.92023091580   [ 0.40268876 -0.22995087 -0.42421415 -0.1464154 ]\n",
      "0.32096833450   [ 0.15860567 -0.16145627  0.04025597 -0.3869026 ]\n",
      "0.33771485079   [-0.30140972  0.27013169 -0.49653246 -0.12583137]\n"
     ]
    }
   ],
   "source": [
    "def main(transition, equilibrium_distribution):\n",
    "    vals = list(zip(np.random.rand(10), np.random.rand(10, 4) - 0.5))\n",
    "    return zip(np.random.rand(2, 4) - 0.5, \n",
    "               [vals[:5], vals[5:]])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transition = np.array([[0.3, 0.1, 0.5, 0.1],\n",
    "                           [0.2, 0.3, 0.15, 0.35],\n",
    "                           [0.25, 0.15, 0.2, 0.4],\n",
    "                           [0.35, 0.2, 0.4, 0.05]])\n",
    "    print(\"Transition probabilities are:\", transition, sep=\"\\n\")\n",
    "    stationary_distributions = get_stationary_distributions(transition)\n",
    "    # Uncomment the below line to check that there actually is only one stationary distribution\n",
    "    # assert len(stationary_distributions) == 1\n",
    "    equilibrium_distribution = stationary_distributions[0]\n",
    "    print(\"Equilibrium distribution:\")\n",
    "    print(equilibrium_distribution)\n",
    "    for initial_distribution, results in main(transition, equilibrium_distribution):\n",
    "        print(\"\\nUsing {} as initial distribution:\".format(initial_distribution))\n",
    "        print(\"kl-divergence   empirical distribution\")\n",
    "        print(\"\\n\".join(\"{:.11f}   {}\".format(di, kl) for di, kl in results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea of solution\n",
    "\n",
    "fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "fill in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "598.85px",
    "left": "1223px",
    "right": "20px",
    "top": "121px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
